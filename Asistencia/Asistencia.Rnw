\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage[letterpaper, margin=1.5in, headheight=16pt]{geometry}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{titling}
\usepackage{float}
\usepackage{xcolor}
\usepackage{hyperref}
\definecolor{cimatred}{RGB}{122,51,69}
\hypersetup{
    colorlinks=true,
    linkcolor=cimatred,
    filecolor=cimatred,      
    urlcolor=cimatred,
    citecolor=cimatred
}
\urlstyle{same}
\newtheorem{theorem}{Teorema}
\newtheorem{corollary}{Corolario}[theorem]
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{definition}{Definición}
\newtheorem{plain}{Proposición}
\newtheorem*{remark}{Observación}
\spanishdecimal{.}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\comp}[1]{#1^\mathrm{C}}
\newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\cov}[1]{\mathrm{cov}\left[#1\right]}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\var}[1]{\mathrm{Var}\left(#1\right)}
\newcommand{\sd}[1]{\mathrm{sd}\left(#1\right)}

\renewcommand{\baselinestretch}{1}
\DeclareMathOperator{\indic} {\textup{\large 1}}
\DeclareMathOperator{\Prob} {\mathbb P}
\DeclareMathOperator{\E} {\mathbb E}
\DeclareMathOperator{\V} {\mathbb V}
\DeclareMathOperator{\Normal} {\mathcal{N}}


\usepackage{fixdif}


\author{José Miguel Saavedra Aguilar}
\title{Ayudantías de Modelos Estadísticos}
\begin{document}

\pagestyle{plain}

\setlength{\parskip}{10pt}
\setlength{\parindent}{5pt}
\begin{minipage}{0.2\linewidth}
\vspace{-1cm}
\includegraphics[width=0.9\linewidth]{logoCIMAT11.png}
\end{minipage}
\begin{minipage}{0.7\linewidth}
\vspace{-1cm}
\noindent {\large \color{cimatred}\textbf{Centro de Investigación en Matemáticas, A.C.}}\\
\textbf{Inferencia Estadística}
\end{minipage}
\vspace{-5mm}
\begin{center}
\textbf{\large \thetitle}\\   %TITULO
\vspace{3mm}
\theauthor
\end{center}
\vspace{-5mm}
\rule{\linewidth}{0.1mm}
<<external-code, cache=FALSE,echo=FALSE>>=
read_chunk('Asistencia.R')
@
\section{Introducción a Inferencia Estadística}

\subsection{Ejemplo 1}

Para una v.a. $X\sim \mathrm{exp}$, graficamos la función de densidad para distintas tasas $\lambda = 5, 2, 0.5$.

<<inic,echo=FALSE,message=FALSE>>=
@

<<t1inicializacion,echo=TRUE>>=
@

<<t1ej1>>=
@
<<t1gr1,echo=FALSE,cache=TRUE, fig.pos="htbp", fig.height=3, fig.width=4.5,fig.align='center', fig.cap="Densidad de una v.a. exponencial para distintas medias">>=
@

Ahora, graficamos las respectivas funciones de distribución de $X$.

<<t1ej2>>=
@

<<t1gr2,echo=FALSE,cache=TRUE, fig.pos="ht", fig.height=2.5, fig.width=4.5,fig.align='center', fig.cap="Distribución exponencial para distintas medias">>=
@


Para $\lambda=0.5$, simulamos una muestra aleatoria de tamaño $n=100$ datos de $X$.

<<t1ej3>>=
@

<<t1gr3,echo=FALSE,cache=TRUE, fig.pos="hb!", fig.height=3, fig.width=4.5,fig.align='center', fig.cap="Histograma de una m.a. Exponencial con $\\lambda=0.5$">>=
@
%Los datos que simulamos tienen media muestral $\Sexpr{mean(z)}$ y varianza muestral $\Sexpr{var(z)}$.\\
Ordenamos los puntos simulados en $x_{(1)},x_{(2)}, \ldots, x_{(n)}$. Les asociamos $k_i$ definido por
\begin{align}
k_i &= \frac{i}{n+1}
\end{align}

<<t1ej4>>=
@
<<t1gr4,echo=FALSE,cache=TRUE, fig.pos="ht", fig.height=2.5, fig.width=4.5,fig.align='center', fig.cap="Comparación entre la función de densidad empírica y la teórica a partir de la muestra">>=
@
Para conocer más sobre la función de distribución empírica, pueden consultar \href{https://en.wikipedia.org/wiki/Empirical_distribution_function}{Wikipedia}.
\section{Ejemplos de Knitr}
Se ejemplifica el uso de knitr. Es recomendable consultar el libro de Yihui Xie \cite{Xie2015} para mayor información.
\subsection{Ejemplo 2}
Una variable aleatoria discreta $X$ tiene función de masa de probabilidad:
$$
\begin{array}{cccccc}{x} & {0} & {1} & {2} & {3} & {4} \\ \hline p(x) & {0.1} & {0.2} & {0.2} & {0.2} & {0.3}\end{array}
$$
Utilicen el teorema de la transformación inversa para generar una muestra aleatoria de tamaño 1000 de la distribución de $X$. Construyan una tabla de frecuencias relativas y comparen las probabilidades empíricas con las teóricas.\\
Repitan considerando la función de R sample.
<<t2ej1>>=
@

\subsection{Ejemplo 3}
Obtengan una muestra de $10,000$ números de la siguiente distribución discreta:
$$
p(x)=\frac{2 x}{k(k+1)}, x=1,2, \ldots, k
$$
para $k=100$
<<t2ej2>>=
@

\subsection{Ejemplo 4}
Una compañía de seguros tiene 1000 asegurados, cada uno de los cuales presentará de manera independiente una reclamación en el siguiente mes con probabilidad $p = 0.09245$. Suponiendo que las cantidades de los reclamos hechos son variables aleatorias Gamma(7000,1), hagan simulación para estimar la probabilidad de que la suma de los reclamos exceda $\$ 500,000$.
<<t2ej3>>=
@

\section{Tarea 3}
\subsection{Ejercicio 1}
Simula las siguientes muestras Poisson, todas con $\lambda = 3$, pero de distintos tamaños, $n = 10,20,40,80,200$. Para cada muestra de estas tres calcula los tres estimadores de momentos dados en las notas en la pág. 3, $\lambda_1 , \lambda_2$ y $\lambda_3$ .
<<t3estimadores>>=
@
<<t3ej2,echo=FALSE>>=
@
\subsection{Ejercicio 2}
Simula una muestra de $n = 15$ variables aleatorias independientes $X_1, ..., X_n$, idénticamente distribuidas como normales con media $\mu = 60$ y parámetro de escala$ \sigma = 5$.\\
<<t3ej3dfn>>=
@
Calcula los estimadores de momentos de $\mu$ y $\sigma$ basados en ecuaciones de los primeros dos momentos, los primeros no centrados y los segundos momentos centrados. Denota a estos estimadores como $\hat{\mu}$y $\hat{\sigma}$.
<<t3ej3a>>=
@
En una misma figura, grafica la función de distribución teórica con línea continua, la distribución estimada con guiones y la función de distribución empírica, graficando puntos de las siguientes coordenadas
$$\left( x_i ,  \frac{i}{n+1} \right)$$
para $i=1,...,n$.
<<t3ej3b,echo=FALSE,cache=TRUE, fig.pos="hb!", fig.height=3, fig.width=4.5,fig.align='center', fig.cap="Funciones de distribución teórica, empírica y estimada, $n=15$">>=
@
Repite lo mismo pero ahora para $n=30$ y luego para $n=100$.
<<t3ej3dfn2,echo=FALSE>>=
@
<<t3ej3c>>=
@
<<t3ej3c1,echo=FALSE,cache=TRUE, fig.pos="hb!", fig.height=3, fig.width=4.5,fig.align='center', fig.cap="Funciones de distribución teórica, empírica y estimada, $n=30$">>=
@
<<t3ej3c2,echo=FALSE,cache=TRUE, fig.pos="hb!", fig.height=3, fig.width=4.5,fig.align='center', fig.cap="Funciones de distribución teórica, empírica y estimada, $n=100$">>=
@

\section{Bootstrap no paramétrico}
Para esta ayudantía, nos basamos en el libro \cite{Wasserman2004}. Haremos bootstrap no paramétrico para estimar los cuantiles 0.025 y 0.975 de una v.a. exponencial. Sea $X\sim \mathrm{exp}(\lambda = 5)$. Iniciamos con una muestra aleatoria de $X$ con $n=300$.
<<t4setBootstrap>>=
@
Para el bootstrap, tomaremos $K=100$ muestras de tamaño $m=30$, de las cuales obtendremos el cuantil empírico 0.025 $q_1^k$ y 0.975 $q_2^k$.
<<t4bootstrap>>=
@
Finalmente, mostramos la estimación de la media y la desviación estándar de los cuantiles $\overline{q_1}$, $\overline{q_2}$, $\sd{q_1}$, $\sd{q_2}$ obtenida por bootstrap no paramétrico.
<<t4comparativo>>=
@

\section{Bootstrap paramétrico}
Haremos bootstrap paramétrico para estimar el cuantil 0.025 y 0.975 de una muestra exponencial. Sea $X\sim \mathrm{exp}(\lambda = 5)$. Recordemos
\begin{align*}
  \mean{X} &= \frac{1}{\lambda}
\end{align*}
Por lo que el estimador de $\lambda$ por el método de momentos es:
\begin{align}
  \hat{\lambda} &= \frac{1}{\mean{X}}
\end{align}
Iniciamos con una muestra aleatoria de $X$ con $n=300$.
<<t5setBootstrap>>=
@
Para el bootstrap, tomaremos $K=100$ muestras de tamaño $m=30$, de las cuales obtendremos el estimador por el método de momentos de cada muestra $\hat{\lambda}_k$, para posteriormente encontrar los cuantiles teórico 0.025 $q_1^k$ y 0.975 $q_2^k$..
<<t5bootstrap>>=
@
Finalmente, mostramos la estimación de la media y la desviación estándar de los cuantiles $\overline{q_1}$, $\overline{q_2}$, $\sd{q_1}$, $\sd{q_2}$ obtenida por bootstrap paramétrico.
<<t5comparativo>>=
@

\section{Intervalos de confianza a partir de bootstrap paramétrico}
Haremos bootstrap paramétrico para estimar intervalos del 95\% de confianza de los cuantiles 0.025 y 0.975 de una muestra exponencial. Sea $X\sim \mathrm{exp}(\lambda = 5)$. Recordemos
\begin{align*}
  \mean{X} &= \frac{1}{\lambda}
\end{align*}
Por lo que el estimador de $\lambda$ por el método de momentos es:
\begin{align}
  \hat{\lambda} &= \frac{1}{\mean{X}}
\end{align}
Iniciamos con una muestra aleatoria de $X$ con $n=300$.
<<t6setBootstrap>>=
@
Para el bootstrap, tomaremos $K=100$ muestras de tamaño $m=30$, de las cuales obtendremos el estimador por el método de momentos de cada muestra $\hat{\lambda}_k$, para posteriormente encontrar los cuantiles teórico 0.025 $q_1^k$ y 0.975 $q_2^k$..
<<t6bootstrap>>=
@
Ahora, mostramos los intervalos de confianza por el método normal de los cuantiles $\overline{q_1}$, $\overline{q_2}$, $\sd{q_1}$, $\sd{q_2}$ obtenidos por bootstrap paramétrico.
<<t6normal>>=
@
Finalmente, mostramos los intervalos de confianza por el método pivotal de los cuantiles $\overline{q_1}$, $\overline{q_2}$, $\sd{q_1}$, $\sd{q_2}$ obtenidos por bootstrap paramétrico.
<<t6pivotal>>=
@
\bibliography{biblio}
\bibliographystyle{IEEEtranS}
\end{document}